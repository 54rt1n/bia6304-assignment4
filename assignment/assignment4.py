# assignment/assignment4.py

"""
# Assignment 4

Name:
Date:
Tools Used:

## Assignment

The following functions are part of a package containing a pipeline for processing and analyzing a dataset of Wikipedia summaries.

The assignment is to complete the implementations of the missing code.

## Submission

Complete this file and submit it to Canvas, as yourname-assignment4.py

The final implementation should implement the two functions below, and have the questions at the end of the file answered.

"""

import dask.dataframe as dd
from gensim import corpora
from gensim.models import LdaModel
from gensim.parsing.preprocessing import STOPWORDS
from gensim.utils import simple_preprocess
from typing import List, Tuple

from .config import Config
from .data.io import load_wikipedia_dataset, save_processed_data
from .data.sampling import uniform_subsample

"""
Setup:

Download a sample of the wikipedia summary dataset from:

https://huggingface.co/datasets/mbukowski/wikipedia-summary-dataset-128k/resolve/main/wikipedia-summary-128k.tsv

It is a subsampling from the full dataset, which at 1.78GB is too large for some of you to download. If you have a large enough
hard drive, you can download the full dataset instead from:

https://huggingface.co/datasets/mbukowski/wikipedia-summary-dataset/resolve/main/wikipedia-summary.parquet

and take advantage of Dask's parallel processing and sharding capabilities to work with the larger dataset, even on a laptop.

NOTE: The full dataset is not required for this assignment. If you do change the dataset, make sure to update the path in your config.

Downloaded files should be placed in the assignment root directory.

"""

def subsampler_pipeline(config: Config):
    """
    Runs a subsampling pipeline on the Wikipedia dataset.
    
    This function loads the Wikipedia dataset from the specified path, performs uniform subsampling on the data to reduce the dataset
    size to the target sample size, and saves the subsampled data.
    
    Args:
        config (Config): A configuration object containing the necessary parameters for the subsampling pipeline.
    
    Raises:
        ValueError: If no data is found in the dataset or the subsampling returns an empty dataset.
    """
        
    raise NotImplementedError("Subsampling pipeline not implemented")


def lda_topic_model(config: Config, corpus: dd.DataFrame) -> Tuple[LdaModel, corpora.Dictionary, List]:
    """
    Constructs an LDA (Latent Dirichlet Allocation) topic model on the provided Wikipedia dataset.

    The function preprocesses/tokenizes the text data, creates a gensim dictionary and bag of words corpus, and trains an LDA model on the corpus.
    
    Args:
        config (Config): A configuration object containing the necessary parameters for the LDA topic model.
        corpus (dd.DataFrame): The Wikipedia dataset to be used for the LDA topic model.
    
    Returns:
        Tuple[LdaModel, corpora.Dictionary, List]: A tuple containing the trained LDA model, the dictionary used to create the corpus, and the bag-of-words corpus.

    Raises:
        ValueError: If no data is found in the dataset.
    """

    raise NotImplementedError("LDA topic model not implemented")


"""
## Questions (2-4 paragraphs each)

### What difficulties did you have with this assignment?

Your answer here

### Did you notice any limitations of the LDA model?

Your answer here

### How might you evaluate the quality of the topics generated by the LDA model?

Your answer here

### How do the different graph layouts capture the relationships between documents and topics?

Your answer here

"""